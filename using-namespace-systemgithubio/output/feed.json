{
    "version": "https://jsonfeed.org/version/1",
    "title": "Using-Namespace-System.github.io",
    "description": "",
    "home_page_url": "https://using-namespace-system.github.io",
    "feed_url": "https://using-namespace-system.github.io/feed.json",
    "user_comment": "",
    "icon": "https://using-namespace-system.github.io/media/website/Capture.PNG",
    "author": {
        "name": "Brian Recktenwall-Calvet"
    },
    "items": [
        {
            "id": "https://using-namespace-system.github.io/natural-language-processing-1-term-document-matrix.html",
            "url": "https://using-namespace-system.github.io/natural-language-processing-1-term-document-matrix.html",
            "title": "Natural Language Processing[1:Code]: Term-Document Matrix",
            "summary": "This is the first in a series of posts on extracting word representations using statistical language modeling techniques. This first installment includes rudimentary corpus preprocessing, tokenization, vectorization, and inferences within the vector space model. The corpus is a public domain dataset of a million news&hellip;",
            "content_html": "<div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput\" data-mime-type=\"text/markdown\">\n<p>This is the first in a series of posts on extracting word representations using statistical language modeling techniques. This first installment includes rudimentary corpus preprocessing, tokenization, vectorization, and inferences within the vector space model. The corpus is a public domain dataset of a million news headlines from the Australian Broadcasting Corporation between 2003 and 2021.</p>\n<p>All code blocks for this part of the project are included in this document. The first block includes the imports used in this part of the project.</p>\n<p><a href=\"https://github.com/Using-Namespace-System/Syntagmatic-And-Paradigmatic-Word-Associations.git\">https://github.com/Using-Namespace-System/Syntagmatic-And-Paradigmatic-Word-Associations.git</a></p>\n<p>The Whole series can be cloned from the link above into a dev container and the configs will include the necessary dependencies.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-CodeCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-CodeMirrorEditor jp-Editor jp-InputArea-editor\" data-type=\"inline\">\n<div class=\"cm-editor cm-s-jupyter\">\n<div class=\"highlight hl-ipython3\">\n<pre><span class=\"kn\">from</span> <span class=\"nn\">itertools</span> <span class=\"kn\">import</span> <span class=\"n\">zip_longest</span>\n<span class=\"kn\">from</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">import</span> <span class=\"n\">figure</span>\n<span class=\"kn\">import</span> <span class=\"nn\">nltk</span>\n<span class=\"kn\">from</span> <span class=\"nn\">nltk.corpus</span> <span class=\"kn\">import</span> <span class=\"n\">stopwords</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.sparse</span> <span class=\"kn\">import</span> <span class=\"n\">csr_array</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.sparse</span> <span class=\"kn\">import</span> <span class=\"n\">find</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pickleshare</span> <span class=\"kn\">import</span> <span class=\"n\">PickleShareDB</span>\n\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">'../input/abcnews-date-text.csv'</span><span class=\"p\">)</span>\n<span class=\"n\">nltk</span><span class=\"o\">.</span><span class=\"n\">download</span><span class=\"p\">(</span><span class=\"s1\">'stopwords'</span><span class=\"p\">)</span>\n<span class=\"n\">stopwords_set</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">stopwords</span><span class=\"o\">.</span><span class=\"n\">words</span><span class=\"p\">(</span><span class=\"s1\">'english'</span><span class=\"p\">))</span>\n</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell-outputWrapper\">\n<div class=\"jp-OutputArea jp-Cell-outputArea\">\n<div class=\"jp-OutputArea-child\">Â </div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput\" data-mime-type=\"text/markdown\">\n<p>Preprocessing the corpus is simplified to filtering out short headlines, small words, and stop-words. Each action is completed in pandas, I believe this may improve readability. The documents are exploded into a single series representing the whole corpus. From here stop-words can be filtered out. No further sanitation is performed.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-CodeMirrorEditor jp-Editor jp-InputArea-editor\" data-type=\"inline\">\n<div class=\"cm-editor cm-s-jupyter\">\n<div class=\"highlight hl-ipython3\">\n<pre><span class=\"c1\">#tokenize and sanitize</span>\n\n<span class=\"c1\">#tokenize documents into individual words</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'tokenized'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">headline_text</span><span class=\"o\">.</span><span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">' '</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#remove short documents from corpus</span>\n<span class=\"n\">df</span><span class=\"p\">[</span><span class=\"s1\">'length'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">)</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">length</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#use random subset of corpus</span>\n<span class=\"n\">df</span><span class=\"o\">=</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">frac</span><span class=\"o\">=</span><span class=\"mf\">0.50</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reset_index</span><span class=\"p\">(</span><span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#flatten all words into single series</span>\n<span class=\"n\">ex</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">explode</span><span class=\"p\">(</span><span class=\"s1\">'tokenized'</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#remove shorter words</span>\n<span class=\"n\">ex</span> <span class=\"o\">=</span> <span class=\"n\">ex</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"n\">ex</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">len</span><span class=\"p\">()</span> <span class=\"o\">&gt;</span> <span class=\"mi\">2</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#remove stop-words</span>\n<span class=\"n\">ex</span> <span class=\"o\">=</span> <span class=\"n\">ex</span><span class=\"o\">.</span><span class=\"n\">loc</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">ex</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">isin</span><span class=\"p\">(</span><span class=\"n\">stopwords_set</span><span class=\"p\">)]</span>\n</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-InputPrompt jp-InputArea-prompt\"><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">Tokenization of the corpus is performed by creating forward and backwards lookup dictionaries. Each unique word is represented as a unique number. This is a very simple method of tokenization.</span></div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-CodeMirrorEditor jp-Editor jp-InputArea-editor\" data-type=\"inline\">\n<div class=\"cm-editor cm-s-jupyter\">\n<div class=\"highlight hl-ipython3\">\n<pre><span class=\"c1\">#create dictionary of words</span>\n\n<span class=\"c1\">#shuffle for sparse matrix visual</span>\n<span class=\"n\">dictionary</span> <span class=\"o\">=</span> <span class=\"n\">ex</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">drop_duplicates</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"n\">frac</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#dataframe with (index/code):word</span>\n<span class=\"n\">dictionary</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">dictionary</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">(),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">'words'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_frame</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#store code:word dictionary for reverse encoding</span>\n<span class=\"n\">dictionary_lookup</span> <span class=\"o\">=</span> <span class=\"n\">dictionary</span><span class=\"o\">.</span><span class=\"n\">to_dict</span><span class=\"p\">()[</span><span class=\"s1\">'words'</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#offset index to prevent clash with zero fill</span>\n<span class=\"n\">dictionary</span><span class=\"p\">[</span><span class=\"s1\">'encode'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dictionary</span><span class=\"o\">.</span><span class=\"n\">index</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n\n<span class=\"c1\">#store word:code dictionary for encoding</span>\n<span class=\"n\">dictionary</span> <span class=\"o\">=</span> <span class=\"n\">dictionary</span><span class=\"o\">.</span><span class=\"n\">set_index</span><span class=\"p\">(</span><span class=\"s1\">'words'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_dict</span><span class=\"p\">()[</span><span class=\"s1\">'encode'</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#use dictionary to encode each word to integer representation</span>\n<span class=\"n\">encode</span> <span class=\"o\">=</span> <span class=\"n\">ex</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"p\">(</span><span class=\"n\">dictionary</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to_frame</span><span class=\"p\">()</span>\n<span class=\"n\">encode</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">'int'</span><span class=\"p\">)</span>\n<span class=\"n\">encode</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">'int'</span><span class=\"p\">)</span>\n<span class=\"c1\">#un-flatten encoded words back into original documents</span>\n<span class=\"n\">docs</span> <span class=\"o\">=</span> <span class=\"n\">encode</span><span class=\"o\">.</span><span class=\"n\">tokenized</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">agg</span><span class=\"p\">(</span><span class=\"nb\">tuple</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#match up document indexes for reverse lookup</span>\n<span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">sort_index</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">iloc</span><span class=\"p\">[</span><span class=\"n\">docs</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">reset_index</span><span class=\"p\">(</span><span class=\"n\">drop</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n<span class=\"n\">docs</span> <span class=\"o\">=</span> <span class=\"n\">docs</span><span class=\"o\">.</span><span class=\"n\">reset_index</span><span class=\"p\">()[</span><span class=\"s1\">'tokenized'</span><span class=\"p\">]</span>\n</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-InputPrompt jp-InputArea-prompt\"><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">In its simplest form the word vector for each term would be the one-hot(binary) encoding of the documents they are (1) and are not (0) present in. Likewise, the transform is comprised of document-word vectors where each is a one-hot encoding of the terms in the corpus that are and are not present in a document.</span></div>\n<div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput\" data-mime-type=\"text/markdown\">\n<p>In this instance the word vector is a count vector. This is similar to one-hot but is able to convey how many times the term occurred in the document.</p>\n<p>For the news headline dataset, document-wise term repetition is minimal and the statistical weight it provides is negligible.</p>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-InputPrompt jp-InputArea-prompt\">InÂ [Â ]:</div>\n<div class=\"jp-CodeMirrorEditor jp-Editor jp-InputArea-editor\" data-type=\"inline\">\n<div class=\"cm-editor cm-s-jupyter\">\n<div class=\"highlight hl-ipython3\">\n<pre><span class=\"c1\">#zero pad x dimension by longest sentence</span>\n<span class=\"n\">encoded_docs</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">zip_longest</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">docs</span><span class=\"o\">.</span><span class=\"n\">to_list</span><span class=\"p\">(),</span> <span class=\"n\">fillvalue</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)))</span>\n\n<span class=\"c1\">#convert to sparse matrix</span>\n<span class=\"n\">encoded_docs</span> <span class=\"o\">=</span> <span class=\"n\">csr_array</span><span class=\"p\">(</span><span class=\"n\">encoded_docs</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#convert to index for each word</span>\n<span class=\"n\">row_column_code</span> <span class=\"o\">=</span> <span class=\"n\">find</span><span class=\"p\">(</span><span class=\"n\">encoded_docs</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#presort by words</span>\n<span class=\"n\">word_sorted_index</span> <span class=\"o\">=</span> <span class=\"n\">row_column_code</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">()</span>\n<span class=\"n\">doc_word</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">row_column_code</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"n\">word_sorted_index</span><span class=\"p\">],</span> <span class=\"n\">row_column_code</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">][</span><span class=\"n\">word_sorted_index</span><span class=\"p\">]])</span>\n\n<span class=\"c1\">#presort by docs and words</span>\n<span class=\"n\">doc_word_sorted_index</span> <span class=\"o\">=</span> <span class=\"n\">doc_word</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">()</span>\n<span class=\"n\">doc_word</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">doc_word</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"n\">doc_word_sorted_index</span><span class=\"p\">],</span> <span class=\"n\">doc_word</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">][</span><span class=\"n\">doc_word_sorted_index</span><span class=\"p\">]])</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">'doc'</span><span class=\"p\">,</span><span class=\"s1\">'word'</span><span class=\"p\">])</span>\n\n<span class=\"c1\">#offset code no longer needed after zero-fill</span>\n<span class=\"n\">doc_word</span><span class=\"o\">.</span><span class=\"n\">word</span> <span class=\"o\">=</span> <span class=\"n\">doc_word</span><span class=\"o\">.</span><span class=\"n\">word</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n\n<span class=\"c1\">#convert to index of word counts per document</span>\n<span class=\"n\">doc_word_count</span>  <span class=\"o\">=</span> <span class=\"n\">doc_word</span><span class=\"o\">.</span><span class=\"n\">groupby</span><span class=\"p\">([</span><span class=\"s1\">'doc'</span><span class=\"p\">,</span><span class=\"s1\">'word'</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to_frame</span><span class=\"p\">(</span><span class=\"s1\">'count'</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reset_index</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">to_numpy</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">T</span>\n\n<span class=\"c1\">#convert to sparse matrix</span>\n<span class=\"n\">sparse_word_doc_matrix</span> <span class=\"o\">=</span> <span class=\"n\">csr_array</span><span class=\"p\">((</span><span class=\"n\">doc_word_count</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],(</span><span class=\"n\">doc_word_count</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">doc_word_count</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])),</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">(</span><span class=\"n\">encoded_docs</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dictionary</span><span class=\"p\">)),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">T</span>\n\n<span class=\"c1\">#visualize sparse matrix</span>\n<span class=\"n\">fig</span> <span class=\"o\">=</span> <span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">sparse_word_doc_matrix_visualization</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_subplot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">sparse_word_doc_matrix_visualization</span><span class=\"o\">.</span><span class=\"n\">spy</span><span class=\"p\">(</span><span class=\"n\">sparse_word_doc_matrix</span><span class=\"p\">,</span> <span class=\"n\">markersize</span><span class=\"o\">=</span><span class=\"mf\">0.007</span><span class=\"p\">,</span> <span class=\"n\">aspect</span> <span class=\"o\">=</span> <span class=\"s1\">'auto'</span><span class=\"p\">)</span>\n\n<span class=\"o\">%</span><span class=\"k\">store</span> sparse_word_doc_matrix\n<span class=\"o\">%</span><span class=\"k\">store</span> dictionary\n<span class=\"o\">%</span><span class=\"k\">store</span> dictionary_lookup\n</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-InputPrompt jp-InputArea-prompt\"><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">The visualization below shows the words (y-axis) and the documents (x-axis) they are in. Across 600000 documents the terms in the corpus that re-occur more regularly form an interesting pattern of lines.<br></span><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\"><br>Out[ ]:<br><br></span><figure class=\"post__image\"><img loading=\"lazy\"  style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://using-namespace-system.github.io/media/posts/2/sparse_matrix.png\" alt=\"\" width=\"848\" height=\"818\" sizes=\"100vw\" srcset=\"https://using-namespace-system.github.io/media/posts/2/responsive/sparse_matrix-xs.png 300w ,https://using-namespace-system.github.io/media/posts/2/responsive/sparse_matrix-sm.png 480w ,https://using-namespace-system.github.io/media/posts/2/responsive/sparse_matrix-md.png 768w ,https://using-namespace-system.github.io/media/posts/2/responsive/sparse_matrix-lg.png 1024w ,https://using-namespace-system.github.io/media/posts/2/responsive/sparse_matrix-xl.png 1360w ,https://using-namespace-system.github.io/media/posts/2/responsive/sparse_matrix-2xl.png 1600w\"></figure><br><br><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">The words that occur together often in the corpus also, as word vectors, are closer together in this 600000 dimensional vector space. This is demonstrated in the table below.<br><br></span><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">In [ ]:</span></div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-CodeCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-CodeMirrorEditor jp-Editor jp-InputArea-editor\" data-type=\"inline\">\n<div class=\"cm-editor cm-s-jupyter\">\n<div class=\"highlight hl-ipython3\">\n<pre><span class=\"c1\">#approximating cosine similarity with dot product of the term document matrix and its transform</span>\n\n<span class=\"n\">similarity_matrix</span>  <span class=\"o\">=</span> <span class=\"n\">sparse_word_doc_matrix</span> <span class=\"o\">@</span> <span class=\"n\">sparse_word_doc_matrix</span><span class=\"o\">.</span><span class=\"n\">T</span>\n\n<span class=\"c1\">#displaying slice of matrix with highest similarity scores</span>\n\n<span class=\"n\">similarity_matrix_compressed</span> <span class=\"o\">=</span> <span class=\"n\">similarity_matrix</span><span class=\"p\">[(</span><span class=\"o\">-</span><span class=\"n\">similarity_matrix</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">()[:</span><span class=\"mi\">6</span><span class=\"p\">]]</span><span class=\"o\">.</span><span class=\"n\">toarray</span><span class=\"p\">()</span>\n\n<span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">((</span><span class=\"o\">-</span><span class=\"n\">similarity_matrix_compressed</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)[:</span><span class=\"mi\">6</span><span class=\"p\">,:</span><span class=\"mi\">6</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">applymap</span><span class=\"p\">(</span><span class=\"n\">dictionary_lookup</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">)</span>\n</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell-outputWrapper\">\n<div class=\"jp-OutputArea jp-Cell-outputArea\">\n<div class=\"jp-OutputArea-child jp-OutputArea-executeResult\">\n<div class=\"jp-OutputPrompt jp-OutputArea-prompt\">Out[Â ]:</div>\n<div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult\" tabindex=\"0\" data-mime-type=\"text/html\">\n<div>\n<table class=\"dataframe\" border=\"1\">\n<thead>\n<tr>\n<th>Â </th>\n<th>0</th>\n<th>1</th>\n<th>2</th>\n<th>3</th>\n<th>4</th>\n<th>5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<th>0</th>\n<td>police</td>\n<td>new</td>\n<td>man</td>\n<td>says</td>\n<td>court</td>\n<td>nsw</td>\n</tr>\n<tr>\n<th>1</th>\n<td>man</td>\n<td>zealand</td>\n<td>charged</td>\n<td>govt</td>\n<td>man</td>\n<td>rural</td>\n</tr>\n<tr>\n<th>2</th>\n<td>investigate</td>\n<td>laws</td>\n<td>police</td>\n<td>minister</td>\n<td>accused</td>\n<td>police</td>\n</tr>\n<tr>\n<th>3</th>\n<td>probe</td>\n<td>police</td>\n<td>court</td>\n<td>australia</td>\n<td>face</td>\n<td>govt</td>\n</tr>\n<tr>\n<th>4</th>\n<td>missing</td>\n<td>cases</td>\n<td>murder</td>\n<td>new</td>\n<td>told</td>\n<td>country</td>\n</tr>\n<tr>\n<th>5</th>\n<td>search</td>\n<td>york</td>\n<td>jailed</td>\n<td>trump</td>\n<td>faces</td>\n<td>coast</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-InputPrompt jp-InputArea-prompt\"><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\"><br>Similarly, the documents with the most similarities are closer together in the vector space.<br><br></span></div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell jp-CodeCell jp-Notebook-cell\">\n<div class=\"jp-Cell-inputWrapper\" tabindex=\"0\">\n<div class=\"jp-InputArea jp-Cell-inputArea\">\n<div class=\"jp-InputPrompt jp-InputArea-prompt\">InÂ [Â ]:</div>\n<div class=\"jp-CodeMirrorEditor jp-Editor jp-InputArea-editor\" data-type=\"inline\">\n<div class=\"cm-editor cm-s-jupyter\">\n<div class=\"highlight hl-ipython3\">\n<pre><span class=\"c1\">#previewing document similarity</span>\n\n<span class=\"n\">doc_similarity_matrix</span>  <span class=\"o\">=</span> <span class=\"n\">sparse_word_doc_matrix</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">@</span> <span class=\"n\">sparse_word_doc_matrix</span>\n\n<span class=\"c1\">#displaying slice of matrix with highest similarity scores</span>\n\n<span class=\"n\">doc_similarity_matrix_compressed</span> <span class=\"o\">=</span> <span class=\"n\">doc_similarity_matrix</span><span class=\"p\">[(</span><span class=\"o\">-</span><span class=\"n\">doc_similarity_matrix</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">()[:</span><span class=\"mi\">6</span><span class=\"p\">]]</span><span class=\"o\">.</span><span class=\"n\">toarray</span><span class=\"p\">()</span>\n\n<span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">((</span><span class=\"o\">-</span><span class=\"n\">doc_similarity_matrix_compressed</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)[:</span><span class=\"mi\">6</span><span class=\"p\">,:</span><span class=\"mi\">6</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">applymap</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">headline_text</span><span class=\"o\">.</span><span class=\"n\">to_dict</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">)</span>\n</pre>\n</div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"jp-Cell-outputWrapper\">\n<div class=\"jp-OutputArea jp-Cell-outputArea\">\n<div class=\"jp-OutputArea-child jp-OutputArea-executeResult\">\n<div class=\"jp-OutputPrompt jp-OutputArea-prompt\">Out[Â ]:</div>\n<div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult\" tabindex=\"0\" data-mime-type=\"text/html\">\n<div>\n<table class=\"dataframe\" border=\"1\">\n<thead>\n<tr>\n<th>Â </th>\n<th>0</th>\n<th>1</th>\n<th>2</th>\n<th>3</th>\n<th>4</th>\n<th>5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<th>0</th>\n<td>man bites police officer on new years day in a...</td>\n<td>police search for man over police van crash</td>\n<td>police search for man 'involved in police chas...</td>\n<td>police union backs new police leadership team</td>\n<td>new south wales government setting up new poli...</td>\n<td>police investigate report of man impersonating...</td>\n</tr>\n<tr>\n<th>1</th>\n<td>new half day public holidays on christmas eve ...</td>\n<td>police search for man 'involved in police chas...</td>\n<td>police search for man over police van crash</td>\n<td>police union demands answers over appointment ...</td>\n<td>new south wales records 1485 new cases of covi...</td>\n<td>police charge man for impersonating officer</td>\n</tr>\n<tr>\n<th>2</th>\n<td>istanbul police arrest new years day gunman and</td>\n<td>police search for police assault suspect</td>\n<td>police search for man over fatal sydney stabbing</td>\n<td>police union slams police commissioner for cho...</td>\n<td>new south wales covid coronavirus five new loc...</td>\n<td>queensland police describe man police believe</td>\n</tr>\n<tr>\n<th>3</th>\n<td>police make new years day drink driving arrests</td>\n<td>police investigate report of man impersonating...</td>\n<td>police investigate report of man impersonating...</td>\n<td>nt police urge examination of new police numbers</td>\n<td>new president for new south wales farmers</td>\n<td>man charged with impersonating police officer</td>\n</tr>\n<tr>\n<th>4</th>\n<td>new police information about tasmanian man mis...</td>\n<td>queensland police describe man police believe</td>\n<td>police to search sydney creek for missing man</td>\n<td>wa police union president harry arnott stood a...</td>\n<td>a new dairy body in new south wales has received</td>\n<td>police search for man 'involved in police chas...</td>\n</tr>\n<tr>\n<th>5</th>\n<td>brother of man fatally stabbed on new years da...</td>\n<td>police investigation after man struck by polic...</td>\n<td>queensland police describe man police believe</td>\n<td>wa police commissioner backs down on new polic...</td>\n<td>new south wales retains top ranking in new two...</td>\n<td>man charge with impersonating police officer a...</td>\n</tr>\n</tbody>\n</table>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>",
            "author": {
                "name": "Brian Recktenwall-Calvet"
            },
            "tags": [
            ],
            "date_published": "2024-01-08T20:53:22-05:00",
            "date_modified": "2024-01-11T05:30:22-05:00"
        }
    ]
}
